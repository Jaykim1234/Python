{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 Tutorial NewsArticles - SVD - LDA",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uks9yb-BbQiR"
      },
      "source": [
        "# Tutorial 2 - Clustering of News Articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUk_UdPrbPES"
      },
      "source": [
        "We will use the dataset [News Articles, from Tianru Dai (2017)](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GMFCTR).\n",
        "\n",
        "This dataset was made for studying political bias in articles, and is made of articles from different sources, reporting on political topics.\n",
        "\n",
        "In this tutorial we will cluster these news articles by topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZfFqUsz6JRH"
      },
      "source": [
        "# Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxecEpNhbPET"
      },
      "source": [
        "!wget https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/GMFCTR/IZQODZ -O NewsArticles.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc3qR1YA6Lal"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kby03mR34VM"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('NewsArticles.csv', encoding='latin1')\n",
        "df = df[['article_id', 'title', 'text']].copy().dropna().reset_index(drop=True)\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uERavaqo5B_F"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11CWdQdLNjF1"
      },
      "source": [
        "df['nb_words'] = df['text'].apply(lambda x: len(x.split()))\n",
        "_ = df['nb_words'].hist(bins=50, figsize=(9, 9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9i8cVNU6ToC"
      },
      "source": [
        "# EXERCISE: Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mdz4t_y7NFk"
      },
      "source": [
        "We need to customize the `K-Means` class from `sklearn`:\n",
        "* Centroids will be the median point of their cluster (instead of the average point)\n",
        "* Distances will be computed as cosine distances (instead of euclidean distances)\n",
        "* cosine distance = 1 - cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aiQ1SBf7VMm"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "class KMedians(KMeans):\n",
        "    def _e_step(self, X):\n",
        "        self.labels_ = cosine_distances(X, self.cluster_centers_).argmin(axis=1)\n",
        "    def _average(self, X):\n",
        "        return np.median(X, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA9_ks3l8cPa"
      },
      "source": [
        "corpus = df['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM5fTzKZ8NVZ"
      },
      "source": [
        "## TODO - BoW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kOYQ9Uh8hrH"
      },
      "source": [
        "Instantiate and fit a CountVectorizer:\n",
        "* english stopwords\n",
        "* lowercase\n",
        "* setup values for `min_df` and `max_df`\n",
        "* decide for a `max_features` value (advice: 50000)\n",
        "* use a token pattern to capture only: 'only letters, and at least 2 letters'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWTbBM2RbjgP"
      },
      "source": [
        "# TODO - BoW\n",
        "# Create a CountVectorizer with the right parameters (hint for token pattern: r'[a-z]{2,}')\n",
        "# fit it to the corpus of texts\n",
        "# transform the corpus into a term-document matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX7Mh3Tm9S1J"
      },
      "source": [
        "Use the term document matrix to cluster the documents:\n",
        "* 8 clusters\n",
        "* Normalize the vectors before clustering (always a good practice)\n",
        "* `KMedians` has the same interface as `KMeans` [Link](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
        "* Might be needed to adjust `max_iter`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpGv8HXoPRbe"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# TODO - Normalize the term-document matrix\n",
        "normalizer = Normalizer()\n",
        "bow_norm = normalizer.fit_transform(# TODO)\n",
        "\n",
        "# TODO -     \n",
        "km = KMedians(\n",
        "    # TODO\n",
        ")\n",
        "km.fit(# TODO)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9dXF33ZVzJi"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Below is some code:\n",
        "* Identify the closest document to a cluster's centroid\n",
        "* Identify the other documents closest to this document\n",
        "\n",
        "**TODO**: observe the titles and evaluate if the articles might be related."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWLmAd0hZCEB"
      },
      "source": [
        "OBSERVE_CLUSTER = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFfvVLSi9h91"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances, pairwise_distances_argmin_min\n",
        "\n",
        "closest, _ = pairwise_distances_argmin_min(X=km.cluster_centers_, Y=bow_norm, metric='cosine')\n",
        "\n",
        "c = closest[OBSERVE_CLUSTER]\n",
        "d = pairwise_distances(X=[km.cluster_centers_[OBSERVE_CLUSTER]], Y=bow_norm, metric='cosine')[0]\n",
        "top_10_idx = np.argsort(d)[1:11]   # the closest to a point is itself, so we remove the TOP 1\n",
        "\n",
        "print(df.iloc[c]['title'])\n",
        "print('*' * 80)\n",
        "for i, idx in enumerate(top_10_idx):\n",
        "    print(f'#{i+1:>2} (idx={idx:4}, d={d[idx]:.2f}): {df.iloc[idx][\"title\"]}')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjhss0dSXTcp"
      },
      "source": [
        "## TODO - SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zpk0xEyXW31"
      },
      "source": [
        "Use SVD to create semantic vectors:\n",
        "* 8 topics\n",
        "* then cluster in 8 clusters\n",
        "* Observe the difference in computation time\n",
        "* Observe the relatedness of articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9leF9EZYbySV"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "K = 8\n",
        "\n",
        "# TODO - Instantiate a TruncatedSVD\n",
        "# Transform the BoW into a document-topic matrix\n",
        "svd = TruncatedSVD(\n",
        "    # TODO\n",
        ")\n",
        "lsa = svd.fit_transform(#TODO - term-doc matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33tux4rybsbq"
      },
      "source": [
        "# TODO - Show the topics and their most important words\n",
        "vocab = yourcountvectorizer.get_feature_names()\n",
        "\n",
        "for topic in range(K):\n",
        "    topic_terms = svd.components_[topic, :]\n",
        "    top_10_indices = topic_terms.argsort()[::-1][:10]\n",
        "    print(f'Topic {topic:>2}: {\"\".join([f\"{vocab[i]:<15}\" for i in top_10_indices])}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_dgS2lXciHH"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# TODO - Normalize the document-topic matrix\n",
        "normalizer = Normalizer()\n",
        "lsa_norm = normalizer.fit_transform(#TODO - LSA)\n",
        "\n",
        "# TODO - Clustering\n",
        "lsa_km = KMedians(\n",
        "    #TODO\n",
        ")\n",
        "\n",
        "lsa_km.fit(#TODO - normalized LSA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMsS06HFZJS4"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Below is some code:\n",
        "* Identify the closest document to a cluster's centroid\n",
        "* Identify the other documents closest to this document\n",
        "\n",
        "**TODO**: observe the titles and evaluate if the articles might be related."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdMxs455ZJS9"
      },
      "source": [
        "OBSERVE_CLUSTER = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9fbpO7uZJS9"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances, pairwise_distances_argmin_min\n",
        "\n",
        "closest, _ = pairwise_distances_argmin_min(X=lsa_km.cluster_centers_, Y=lsa_norm, metric='cosine')\n",
        "\n",
        "c = closest[OBSERVE_CLUSTER]\n",
        "d = pairwise_distances(X=[lsa_km.cluster_centers_[OBSERVE_CLUSTER]], Y=bow_norm, metric='cosine')[0]\n",
        "top_10_idx = np.argsort(d)[1:11]   # the closest to a point is itself, so we remove the TOP 1\n",
        "\n",
        "print(df.iloc[c]['title'])\n",
        "print('*' * 80)\n",
        "for i, idx in enumerate(top_10_idx):\n",
        "    print(f'#{i+1:>2} (idx={idx:4}, d={d[idx]:.2f}): {df.iloc[idx][\"title\"]}')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ37gf47bgvj"
      },
      "source": [
        "## TODO - LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmpWY2p7bi7c"
      },
      "source": [
        "Use LDA to create semantic vectors:\n",
        "* 8 topics\n",
        "* Same as SVD\n",
        "* Take inspiration in the LDA Notebook of the lecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RB0p8ESbyOn"
      },
      "source": [
        "# Create tokenized corpus\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "add_stops = ['said', 'mr']\n",
        "\n",
        "stopped_tokenized = list(map(\n",
        "    lambda tokens: [t.text for t in tokens if len(t.text) > 1 and not t.is_stop and t.text not in add_stops],\n",
        "    nlp.tokenizer.pipe(df['text'])\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgKygxTIg1qS"
      },
      "source": [
        "# TODO - Create the corpus (see Classroom notebook)\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "dictionary = Dictionary(stopped_tokenized)\n",
        "corpus = [dictionary.doc2bow(txt) for txt in stopped_tokenized]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EKWTHNPb_J7"
      },
      "source": [
        "# TODO - Create the LDA model with 8 topics\n",
        "# Evaluate it\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "lda = LdaModel(\n",
        "    # TODO,\n",
        "    minimum_probability=0.0\n",
        ")\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_cv = CoherenceModel(\n",
        "    # TODO\n",
        "    coherence='c_v'\n",
        ")\n",
        "c_v = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzy5UDxecDQT"
      },
      "source": [
        "# TODO - Display the topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkQuYiQcglDk"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "lda_vecs = lda[corpus]\n",
        "\n",
        "# Create the document-topic matrix\n",
        "doc_topics = np.zeros((len(corpus), K))\n",
        "for i in range(len(corpus)):\n",
        "    topics = lda_vecs[i][0]\n",
        "    for (j, v) in topics:\n",
        "        doc_topics[i][j] = v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUghQnU7cGL6"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# TODO - Normalize the document-topic matrix\n",
        "normalizer = Normalizer()\n",
        "lda_norm = normalizer.fit_transform(# TODO - document/topic matrix)\n",
        "\n",
        "# TODO - Clustering\n",
        "lda_km = KMedians(\n",
        "    # TODO\n",
        ")\n",
        "\n",
        "lda_km.fit(# TODO - use the normalized document/topic matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6c79lWaikae"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Below is some code:\n",
        "* Identify the closest document to a cluster's centroid\n",
        "* Identify the other documents closest to this document\n",
        "\n",
        "**TODO**: observe the titles and evaluate if the articles might be related."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol0J3ZOMikap"
      },
      "source": [
        "OBSERVE_CLUSTER = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oHpFCLhikaq"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances, pairwise_distances_argmin_min\n",
        "\n",
        "closest, _ = pairwise_distances_argmin_min(X=lda_km.cluster_centers_, Y=lda_norm, metric='cosine')\n",
        "\n",
        "c = closest[OBSERVE_CLUSTER]\n",
        "d = pairwise_distances(X=[lda_km.cluster_centers_[OBSERVE_CLUSTER]], Y=bow_norm, metric='cosine')[0]\n",
        "top_10_idx = np.argsort(d)[1:11]   # the closest to a point is itself, so we remove the TOP 1\n",
        "\n",
        "print(df.iloc[c]['title'])\n",
        "print('*' * 80)\n",
        "for i, idx in enumerate(top_10_idx):\n",
        "    print(f'#{i+1:>2} (idx={idx:4}, d={d[idx]:.2f}): {df.iloc[idx][\"title\"]}')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppjsTxyDiooi"
      },
      "source": [
        "## TODO - LDA GridSearch with Coherence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHc6HEWqiws0"
      },
      "source": [
        "* Find the optimum number of topics by maximizing the coherence score $\\textrm{C}_V$\n",
        "* Display these topics\n",
        "* Explore the clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCfwkA3PiwF-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}