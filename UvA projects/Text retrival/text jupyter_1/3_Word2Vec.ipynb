{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USAYnCRGJO2i"
   },
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EnbZEP2wdhn"
   },
   "source": [
    "# Author\n",
    "Julien ROSSI for the University of Amsterdam (2021).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "067nMMnueMZq"
   },
   "source": [
    "Word2Vec is a model described by Mikolov et al in 2013, it is as well a patented algorithm by Google:\n",
    "* \"Efficient Estimation of Word Representations in Vector Space\" [ArXiv](https://arxiv.org/abs/1301.3781)\n",
    "* \"Distributed representations of words and phrases and their compositionality\" [ArXiv](https://arxiv.org/abs/1310.4546)\n",
    "* \"Computing numeric representations of words in a high-dimensional space\" [Patent](https://patents.google.com/patent/US9037464B1/en)\n",
    "\n",
    "A neural network with 1 hidden layer trains on the task of predicting a word given a few context words:\n",
    "* For example, with a window of size 5\n",
    "* The sample is a part of a sentence \"my blue ship sails faster\"\n",
    " * Context words: `my` `blue` `sails` `faster`\n",
    " * Central word: `ship`\n",
    "* **Skip-Gram**: predict `my` `blue` `sails` `faster` from `ship`\n",
    "* From a complete corpus, extract as many samples as possible\n",
    "* The sample loss is the difference between predicted probabilities of each word of the dictionary versus ground truth (log likelyhood)\n",
    "* Minimize the loss over all the dataset\n",
    "\n",
    "Once the neural network is trained:\n",
    "* Read the weights of the hidden layer as word embeddings\n",
    "* This is also the values in the neurons of the hidden layer when the word is given as input (green area on the illustration)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*HQeN5Q9FhN_XPbM4QuWIRg.jpeg\"></img>\n",
    "\n",
    "Image source: https://medium.com/@zeeshanmulla/word-embeddings-in-natural-language-processing-nlp-5be7d6fb1d73\n",
    "\n",
    "The contribution of Mikolov et al. deals mainly with optimizations of the training so that it is actually tractable. We will not enter into these details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFwuc5FFbz_R"
   },
   "source": [
    "# Use an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vUZKd-Xb2t5"
   },
   "source": [
    "Considering the effort, it is worth using a pretrained model.\n",
    "\n",
    "What is a pretrained model:\n",
    "* a dictionary\n",
    "* each key is a word\n",
    "* each value is a vector\n",
    "\n",
    "**Warning**\n",
    "\n",
    "It will download **1.6GB** of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IijFUUt0W3SD"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "re5XOxOTY8hj"
   },
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQIVJAcnfIe3"
   },
   "outputs": [],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxNlpZTKcQ35"
   },
   "source": [
    "Have a look at vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_7jKnaYaW1U"
   },
   "outputs": [],
   "source": [
    "model['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bebeHpCNbP3c"
   },
   "outputs": [],
   "source": [
    "model['sklsajhdgfjkhsosiuerhksjdhfkjsh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBsC9LOGcUdZ"
   },
   "source": [
    "## Most similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71QRxw9-gzRt"
   },
   "source": [
    "The similarity between words is computed as the cosine similarity between the vectors representing these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0-WruQtfSxy"
   },
   "outputs": [],
   "source": [
    "model.similarity('investment', 'flower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qPgl4PDa3_D"
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5cakuhAg9TW"
   },
   "source": [
    "## Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InPiSvqpgGQB"
   },
   "source": [
    "There are a few known vector equations, like:\n",
    "\n",
    "$\\overrightarrow{\\textrm{king}} - \\overrightarrow{\\textrm{man}} + \\overrightarrow{\\textrm{woman}} = \\overrightarrow{\\textrm{queen}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNLOc-2EfdzK"
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yX11mCr2hGgS"
   },
   "source": [
    "$\\overrightarrow{\\textrm{paris}} - \\overrightarrow{\\textrm{france}} + \\overrightarrow{\\textrm{germany}} = \\overrightarrow{\\textrm{berlin}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFjU1Y-ofxI2"
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['paris', 'germany'], negative=['france'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqZeElorJRvO"
   },
   "source": [
    "# Training with a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ekrZdbnyAtx"
   },
   "source": [
    "We will use the [Brown Corpus](http://korpus.uib.no/icame/manuals/BROWN/INDEX.HTM) as illustration.\n",
    "\n",
    "This corpus is made of books published in 1961, written by native English speakers.\n",
    "\n",
    "We will generate 100-dims vector for the words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GMheswfJLl1"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIdbU5rvJ0yt"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import BrownCorpus\n",
    "\n",
    "brown = BrownCorpus('/root/nltk_data/corpora/brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlwhC7Cry9hr"
   },
   "source": [
    "It is a list of tokenized sentences. Each word his also flagged with its Part-of-Speech tag (POS). \n",
    "\n",
    "* `pp` = personal pronoun\n",
    "* `vb` = verb\n",
    "* etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BD5i4WE_ytOK"
   },
   "outputs": [],
   "source": [
    "all_brown = list(brown)\n",
    "print(all_brown[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CN2cytZgzSac"
   },
   "outputs": [],
   "source": [
    "print(f'Brown Corpus contains {len(all_brown)} sentences, and a total of {sum(map(len, brown))} tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzoKRKD5KFaq"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=brown,\n",
    "    size=100,\n",
    "    window=3,\n",
    "    iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9xA7jmHz2tt"
   },
   "outputs": [],
   "source": [
    "print(f'Word2Vec created for a vocabulary of {len(w2v.wv.vocab)} unique terms.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Anvn5P6SLJdS"
   },
   "outputs": [],
   "source": [
    "w2v.wv['happy/jj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uuov2v5L0F8e"
   },
   "source": [
    "Now we can evaluate and see that it is not performing well. \n",
    "\n",
    "We would need:\n",
    "* More data\n",
    "* More processing to train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kpivj_AXMbJC"
   },
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(positive=['happy/jj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIkI2vvl1SXp"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOQV_Nof1TvR"
   },
   "source": [
    "Evaluation is conducted by checking if a list in similarities in words (given by human) are reflected well as similarities in between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "It_8WtgSx4eR"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "w2v.wv.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZM5R8_Qb04xv"
   },
   "outputs": [],
   "source": [
    "w2v.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1cD9xtC10sZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Word2Vec",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
